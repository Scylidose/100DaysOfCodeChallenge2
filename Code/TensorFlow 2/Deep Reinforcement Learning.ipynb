{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "## Definition\n",
    "\n",
    "Uses **deep learning** and **reinforcement learning** principles in order to create efficient algorithms that can be applied on areas like robotics, video games, finance, healthcare.  \n",
    "\n",
    "## Reinforcement learning task\n",
    "\n",
    "Training an **agent** which interacts with its **environment**. The agent arrives at different scenarios known as **states** by performing actions. **Actions** lead to rewards which could be positive and negative.  \n",
    "\n",
    "The agent has only one purpose here ‚Äì to **maximize its total reward** across an **episode**. This episode is anything and everything that happens between the first state and the last or terminal state within the environment. We reinforce the agent to learn to **perform the best actions** by experience. This is the **strategy** or **policy**.  \n",
    "\n",
    "## The Bellman Equation\n",
    "\n",
    "It writes the *value* of a decision problem at a certain point in time in terms of the payoff from some initial choices and the *value* of the remaining decision problem that results from those initial choices.\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- s = State  \n",
    "- a = Action  \n",
    "- R = Reward  \n",
    "- Y = Discount  \n",
    "- V = Value of a state  \n",
    "\n",
    "### Equation \n",
    "\n",
    "![equation](https://i.imgur.com/I39aaIa.png)\n",
    "\n",
    "- s' = New State\n",
    "\n",
    "#### Dicount Factor\n",
    "\n",
    "Determines the importance of future rewards.\n",
    "\n",
    "## Markov Decision Process\n",
    "\n",
    "**Discrete time stochastic control** process.  \n",
    "- Discrete Time : Time series consisting of a sequence of quantities.  \n",
    "- Stochastic : Randomly determined process.  \n",
    "- Control : Deals with finding a control law for a dynamical system over a period of time such that an objective function is optimized.  \n",
    "\n",
    "### Markov Property\n",
    "State **depends** solely on the **previous** state and the **transition** from that state to the current state.  \n",
    "\n",
    "### Equation\n",
    "\n",
    "![equation](https://i.imgur.com/K7ILfti.png)\n",
    "\n",
    "## Q-Learning\n",
    "Finds a policy that is optimal in the sense that it maximizes the expected value of the total reward over any and all successive steps, starting from the current state.  \n",
    "\n",
    "Perform the sequence of actions that will eventually generate the maximum total reward.  \n",
    "\n",
    "- Q = Value of an action  \n",
    "\n",
    "![formula](https://i.imgur.com/XmPfsqi.png)\n",
    "\n",
    "## Temporal Difference\n",
    "\n",
    "Adjust predictions to match later, more accurate, predictions about the future before the final outcome is known.  \n",
    "\n",
    "![formula](https://i.imgur.com/L0SJiqe.png)\n",
    "\n",
    "![formula2](https://i.imgur.com/HLEQd20.png)\n",
    "\n",
    "### Full formula \n",
    "\n",
    "![formula3](https://i.imgur.com/2TvtcCc.png)\n",
    "\n",
    "- Œ± = Learning Rate  \n",
    "\n",
    "#### Learning Rate\n",
    "\n",
    "Hyperparameter which determines to what extent newly acquired information overrides old information.\n",
    "\n",
    "## Deep Q-Learning\n",
    "\n",
    "When the cheatsheet of the action is too long.  \n",
    "\n",
    "Use a **neural network** to approximate the Q-value function.  \n",
    "\n",
    "The state is given as the input and the Q-value of all possible actions is generated as the output.  \n",
    "\n",
    "![qlearning vs deepQL](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM-850x558.png)\n",
    "\n",
    "1) All the past experience is stored by the user in memory  \n",
    "2) The next action is determined by the maximum output of the Q-network  \n",
    "3) The loss function here is mean squared error of the predicted Q-value and the target Q-value ‚Äì Q*. This is basically a regression problem. However, we do not know the target or actual value here as we are dealing with a reinforcement learning problem. Going back to the Q-value update equation derived from the *Bellman equation*.  \n",
    "\n",
    "#### 1- Loss Function \n",
    "\n",
    "![loss](https://i.imgur.com/c7eQbkV.png)\n",
    "\n",
    "#### 2- Selecting the right Q \n",
    "\n",
    "Select the action with the highest Q-value using SoftMax algorithm.  \n",
    "\n",
    "![softmax](https://i.imgur.com/dgHJB4q.png)\n",
    "\n",
    "## Experience Replay\n",
    "\n",
    "Maintain a **buffer** of the old experiences and train it again on them.  \n",
    "\n",
    "Store the (S,A) = R situations that create the highest ‚Äúsurprise‚Äù (highest loss).  \n",
    "\n",
    "- Advantages of experience replay:\n",
    "\n",
    "    - More efficient use of previous experience, by learning with it multiple times. This is key when gaining real-world experience is costly, you can get full use of it. The Q-learning updates are incremental and do not converge quickly, so multiple passes with the same data is beneficial, especially when there is low variance in immediate outcomes (reward, next state) given the same state, action pair.\n",
    "\n",
    "    - Better convergence behaviour when training a function approximator. Partly this is because the data is more like data assumed in most supervised learning convergence proofs.\n",
    "\n",
    "- Disadvantage of experience replay:\n",
    "\n",
    "    - It is harder to use multi-step learning algorithms, such as Q(ùúÜ), which can be tuned to give better learning curves by balancing between bias (due to bootstrapping) and variance (due to delays and randomness in long-term outcomes).\n",
    "\n",
    "## Action Selection Policies\n",
    "\n",
    "- Epsilon-greedy : The best lever is selected for a proportion 1 - epsilon of the trials, and a lever is selected at random (with uniform probability) for a proportion epsilon.\n",
    "\n",
    "- Epsilon-soft (1-Epsilon) : Selecting at random 1 - epsilon. \n",
    "\n",
    "- Softmax : Takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities  \n",
    "\n",
    "### Exploration vs Exploitation\n",
    "- Exploration : Where we gather more information that might lead us to better decisions in the future. Don't end up in local maximum.  \n",
    "\n",
    "- Exploitation : Where we make the best decision given current information.  \n",
    "\n",
    "## Applications \n",
    "\n",
    "![applications](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/04/Screenshot-2019-04-17-at-3.48.22-PM-850x626.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
