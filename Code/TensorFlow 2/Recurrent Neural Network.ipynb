{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "Can take a series of input with **no predetermined limit** on size.  \n",
    "\n",
    "Can take one or more input vectors and produce one or more output vectors and the output(s) are **influenced** not just by **weights** applied on inputs like a regular NN, but also by a **hidden state** vector representing the **context** based on prior input(s)/output(s).  \n",
    "\n",
    "![rnn](https://miro.medium.com/max/1260/1*aIT6tmnk3qHpStkOX3gGcQ.png)\n",
    "\n",
    "![rnn2](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "\n",
    "![types](https://i.imgur.com/yweicB5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN\n",
    "Four possible ways to add depth :  \n",
    "1) Add **hidden states**, one on top of another, feeding the output of one to the next.  \n",
    "2) We can also add additional **nonlinear hidden layers** between input to hidden state.  \n",
    "3) We can increase depth in the **hidden to hidden** transition.  \n",
    "4) We can increase depth in the **hidden to output** transition.  \n",
    "\n",
    "## Bidirectional RNN\n",
    "**Look into the future to fix the past.**  \n",
    "\n",
    "![bidirectional RNN](https://miro.medium.com/max/1260/1*4boTkuSnOzkVfsvatgYthQ.png)\n",
    "\n",
    "## Recursive Neural Network\n",
    "The transitions are repeatedly applied to inputs, but **not necessarily in a sequential fashion**.  \n",
    "It can operate on any hierarchical tree structure.  \n",
    "Parsing through input nodes, combining child nodes into parent nodes and combining them with other child/parent nodes to create a tree like structure.  \n",
    "\n",
    "![recursive RNN](https://miro.medium.com/max/1260/1*IbpHou3FVc5Mfw4t6XzL6w.png)\n",
    "\n",
    "## Encoder Decoder Sequence to Sequence RNN\n",
    "Used a lot in translation services.  \n",
    "Two RNNs :  \n",
    "- One an **encoder** that keeps updating its hidden state and produces a final single “Context” output.  \n",
    "- This is then fed to the **decoder**, which translates this context to a sequence of outputs.  \n",
    "\n",
    "![edss RNN](https://miro.medium.com/max/1260/1*EtPN2quUtNhl156ebppRPQ.png)\n",
    "\n",
    "## Vanishing gradient\n",
    "\n",
    "![vanishing gradient](https://i.imgur.com/5iQdihD.png)\n",
    "\n",
    "## Long Short Term Memory Networks \n",
    "Capable of learning **long-term dependencies**.  \n",
    "\n",
    "LSTMs have a chain of repeating modules of neural network. Instead of having a single neural network layer, there are four, interacting in a very special way.\n",
    "\n",
    "![lstm](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "![lstm legende](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png)\n",
    "\n",
    "In the above diagram  \n",
    "- Each line carries an entire vector, from the output of one node to the inputs of others.  \n",
    "- The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers.  \n",
    "- Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.  \n",
    "\n",
    "### Step by Step\n",
    "\n",
    "The key to LSTMs is the **cell state**, the **horizontal line running through the top** of the diagram.  \n",
    "\n",
    "The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.  \n",
    "\n",
    "1) Decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1 and xt, and outputs a number between 0 and 1 for each number in the cell state Ct−1. A 1 represents “completely keep this” while a 0 represents “completely get rid of this.”  \n",
    "![step1](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
    "\n",
    "2) Decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, C̃ t, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.  \n",
    "![step2](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
    "\n",
    "3) Update the old cell state, Ct−1, into the new cell state Ct. The previous steps already decided what to do, we just need to actually do it. We multiply the old state by ft, forgetting the things we decided to forget earlier. Then we add it∗C̃ t. This is the new candidate values, scaled by how much we decided to update each state value.  \n",
    "![step3](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)\n",
    "\n",
    "4) Decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.  \n",
    "![step4](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
