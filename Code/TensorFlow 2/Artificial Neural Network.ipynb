{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "\n",
    "Use many layers of **nonlinear processing** units for **feature extraction and transformation**.  \n",
    "Each successive layer **uses the output of the previous layer** for its input.  \n",
    "What they learn forms a **hierarchy of concepts**.  \n",
    "In this hierarchy, each level learns to transform its input data into a more and more abstract and composite representation.  \n",
    "\n",
    "![neuron](https://miro.medium.com/max/1260/1*L_lfAEddxxAg2EqJfB5i6Q.png)\n",
    "\n",
    "![ann](https://miro.medium.com/max/1800/1*l57B0pjXoO-1H1xZYV7QBA.png)\n",
    "\n",
    "1) Based on the connection strength (**weights**) and **transfer function**, the activation value passes to the next node.  \n",
    "2) Each of the nodes **sums the activation values** that it receives (it calculates the **weighted sum**) and modifies that sum based on its transfer function.  \n",
    "3) Next, it applies an **activation function** (function that’s applied to this particular neuron).  \n",
    "4) The neuron understands if it needs to pass along a signal or not.  \n",
    "5) The activation runs through the network until it reaches the output nodes.  \n",
    "6) The output nodes then give us the information in a way that we can understand.  \n",
    "\n",
    "\n",
    "The **model performance** is evaluated by the **cost function**. \n",
    "Your network will use a **cost function** (minimize loss function) to **compare** the output and the actual expected output.  \n",
    "\n",
    "7) **Back propagation :** The information goes back, and the neural network begins to learn with the goal of minimizing the cost function by tweaking the weights.  \n",
    "\n",
    "### Forward Propagation\n",
    "Information is entered into the input layer and **propagates forward** through the network to get our output values.\n",
    "\n",
    "## Activation Function\n",
    "**Translates** the input signals to output signals.  \n",
    "**Maps** the output values on a range like 0 to 1 or -1 to 1\n",
    "\n",
    "- **Threshold Function :** If the summed value of the input reaches a certain threshold the function passes on 0. If it’s equal to or more than zero, then it would pass on 1.\n",
    "\n",
    "![threshold](https://miro.medium.com/max/259/1*DC237QrcxQtCa5Z5ueTVrw.png)\n",
    "\n",
    "- **Sigmoid Function :** **Smooth, gradual progression** from 0 to 1. It’s very useful in the output layer and is heavily used for **linear regression**.\n",
    "\n",
    "![sigmoid](https://miro.medium.com/max/259/1*VlLJGjp2N97E1T2BcCI4Hg.png)\n",
    "\n",
    "- **Hyperbolic Tangent Function :** The value goes below zero, from -1 to 1.\n",
    "\n",
    "![hyperbolic](https://miro.medium.com/max/259/1*XRqAB63J8SZ8EmQsCqnx4Q.png)\n",
    "\n",
    "- **Rectifier Function :** **Smooth and gradual** after the kink at 0. This means, for example, that your output would be either *no* or a **percentage** of *yes*.  \n",
    "*most efficient and biologically plausible*\n",
    "\n",
    "![rectifier](https://miro.medium.com/max/259/1*clkGLXsbu4P0RDf5IWcj_g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted Weights \n",
    "Use a **gradient descent** to look at the angle of the slope of the weights and find out if it's positive or negative in order to continue to slope downhill to find the best weights to reach the **global minimum**.  \n",
    "\n",
    "### Gradient Descent \n",
    "Algorithm for finding the **minimum of a function**.  \n",
    "The machine is **learning the gradient**, or *direction*, that the model should take to **reduce errors**.\n",
    "\n",
    "![gradient](https://miro.medium.com/max/366/1*kmmjFBP5vRkKOM1SP4URpA.png) \n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "Normal gradient descent (or **batch gradient descent**) will get **stuck at a local minimum** rather than a global minimum.\n",
    "![local minimum](https://miro.medium.com/max/536/1*b7Gpub8q4zVXdv4GcBSK3A.png)\n",
    "\n",
    "To counter that, we use the **Stochastic Gradient Descent** which take the rows one by one, run the neural networks, look at the cost functions, adjust de weights, and then move to the next row. (**Adjusting** the weights for each row)  \n",
    "Has much **higher fluctuations**, which allows you to find the global minimum.  \n",
    "\n",
    "#### Mini-Batch Gradient Descent\n",
    "Set a number of rows, run that many rows at a time, and then update your weights.  \n",
    "\n",
    "![gradient comparaison](https://cdn-images-1.medium.com/max/1600/1*3L2t1Da4M3ztbB0I1Torhw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an artificial neural network with stochastic gradient descent\n",
    "\n",
    "1) Randomly initiate weights to small numbers close to 0.  \n",
    "\n",
    "2) Input the first observation of your dataset into the input layer, with each feature in one input node.  \n",
    "\n",
    "3) **Forward propagation** — from left to right, the neurons are activated in a way that each neuron’s activation is limited by the weights. You propagate the activations until you get the predicted result.  \n",
    "\n",
    "4) Compare the predicted result to the actual result and measure the generated error.  \n",
    "\n",
    "5) **Backpropagation** — from right to left, the error is back propagated. The weights are updated according to how much they are responsible for the error. (The learning rate decides how much we update the weights.)  \n",
    "\n",
    "6) **Reinforcement learning** (repeat steps 1–5 and update the weights after each observation) OR batch learning (repeat steps 1–5, but update the weights only after a batch of observations).  \n",
    "\n",
    "7) When the whole training set has passed through the ANN, that is **one epoch**. Repeat with more epochs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "### Classification\n",
    "Binary decisions or multiple-class identification in which observations are separated into categories according to specified. Use *cross sectional* data.  \n",
    "- Credit card fraud detection reportedly  \n",
    "- Cursive handwriting recognition  \n",
    "- Cervical smear screening system  \n",
    "- Petroleum exploration (determine locations of underground oil and gas deposits)  \n",
    "- Detection of bombs in suitcases  \n",
    "\n",
    "### Time Series  \n",
    "Build a **forecasting** model from the historical data set to predict future data points.  \n",
    "- Foreign exchange trading systems  \n",
    "- Portfolio selection and management  \n",
    "- Forecasting weather patterns  \n",
    "- Speech recognition network being marketed  \n",
    "- Predicting/confirming myocardial infarction, a heart attack, from the output waves of an electrocardiogram (ECG)  \n",
    "- Identifying dementia from analysis of electrode-electroencephalogram  \n",
    "\n",
    "### Optimization  \n",
    "Finding solution for a set of very difficult problems known as Non-Polynomial **(NP)-complete** problems  \n",
    "- Traveling salesman problem  \n",
    "- Job-scheduling in manufacturing and efficient routing problems involving vehicles or telecommunication.  \n",
    "\n",
    "![application](https://i.imgur.com/ufr7Ywr.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
