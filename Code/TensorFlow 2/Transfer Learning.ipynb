{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning and Fine Tuning\n",
    "\n",
    "## Transfer Learning\n",
    "A model is developed for a task and **reused** as the **starting point** for a model on a second task.  \n",
    "\n",
    "Transfer learning is an **optimization** that allows **rapid progress** or **improved performance** when modeling the second task.  \n",
    "\n",
    "In transfer learning, we first **train a base network** on a base dataset and task, and then we **repurpose** the learned features, or transfer them, to a second target network to be **trained** on a target dataset and task.  \n",
    "This process will tend to work if the features are **general, meaning suitable** to both base and target tasks, instead of specific to the base task.  \n",
    "\n",
    "### Approaches in Transfer Learning\n",
    "#### Develop Method \n",
    "1) **Select Source Task**. You must select a related predictive modeling problem with an abundance of data where there is some relationship in the input data, output data, and/or concepts learned during the mapping from input to output data.  \n",
    "2) **Develop Source Model**. Next, you must develop a skillful model for this first task. The model must be better than a naive model to ensure that some feature learning has been performed.  \n",
    "3) **Reuse Model**. The model fit on the source task can then be used as the starting point for a model on the second task of interest. This may involve using all or parts of the model, depending on the modeling technique used.  \n",
    "4) **Tune Model**. Optionally, the model may need to be adapted or refined on the input-output pair data available for the task of interest.  \n",
    "\n",
    "#### Pre-trained Model\n",
    "1) **Select Source Model**. A pre-trained source model is chosen from available models. Many research institutions release models on large and challenging datasets that may be included in the pool of candidate models from which to choose from.  \n",
    "2) **Reuse Model**. The model pre-trained model can then be used as the starting point for a model on the second task of interest. This may involve using all or parts of the model, depending on the modeling technique used.  \n",
    "3) **Tune Model**. Optionally, the model may need to be adapted or refined on the input-output pair data available for the task of interest.  \n",
    "\n",
    "### When to use \n",
    "#### Use on Small and Similar dataset\n",
    "1) **Higher start**. The initial skill (before refining the model) on the source model is higher than it otherwise would be.  \n",
    "2) **Higher slope**. The rate of improvement of skill during training of the source model is steeper than it otherwise would be.  \n",
    "3) **Higher asymptote**. The converged skill of the trained model is better than it otherwise would be.  \n",
    "\n",
    "\n",
    "![comparaison](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Three-ways-in-which-transfer-might-improve-learning.png)\n",
    "\n",
    "#### After the Transfer Learning is done, we freeze the Base Model\n",
    "\n",
    "## Fine Tuning\n",
    "In Fine Tuning we only **freeze half** of the Base Model, the rest of the layers are unfrozen and ready to be changed according to the custom data set.  \n",
    "\n",
    "### When to use\n",
    "1) **Large** and **similar** dataset.  \n",
    "2) **Small** and **different** dataset.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
