{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networt\n",
    "\n",
    "A CNN **convolves** learned features with input data and uses 2D convolutional layers.  \n",
    "\n",
    "They’re most commonly used to **analyze visual imagery** and are frequently working behind the scenes in **image classification**.  \n",
    "\n",
    "## 1- Convolution\n",
    "\n",
    "Shows how one function modify the shape of an another function.  \n",
    "\n",
    "![formula](https://wikimedia.org/api/rest_v1/media/math/render/svg/09e0e66216b29a0ccd3d60a6e0f9aba3ce6fb4b2)  \n",
    "\n",
    "![convolution](https://upload.wikimedia.org/wikipedia/commons/c/c6/Convolucion_Funcion_Pi.gif)\n",
    "\n",
    "In CNN it consist of **extracting features** from the input image with the help of a **feature detector**, and a **feature map**.  \n",
    "![feature_cnn](https://media.giphy.com/media/i4NjAwytgIRDW/giphy.gif)\n",
    "\n",
    "You’ll specify parameters like the **number of filters**, the **filter size**, the **architecture** of the network, ...  \n",
    "Learns the values of the filters on its own during the training process.  \n",
    "\n",
    "- **Wide Convolution :** Choose to pad the input matrix with zeros (*zero padding*) to apply the filter to bordering elements of the input image matrix, allowing you to control the size of the feature maps.  \n",
    "- **Narrow Convolution :** Not adding zero padding.  \n",
    "\n",
    "We don’t look at every single pixel of an image. We see **features**.  \n",
    "### The result of this is the *convolved feature map*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - ReLu (Rectified Linear unit) Layer\n",
    "\n",
    "![relu](https://miro.medium.com/max/714/1*oePAhrm74RNnNEolprmTaQ.png)\n",
    "\n",
    "Applying an **activation function** onto your feature maps to **increase non-linearity** in the network.  \n",
    "This is because images themselves are highly non-linear.  \n",
    "It **removes negative** values from an activation map by setting them to zero.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Max Pooling \n",
    "You want **spatial variance**! You want **flexibility**.  \n",
    "\n",
    "Pooling progressively reduces the size of the input representation. It makes it possible to detect objects in an image no matter where they’re located.  \n",
    "It also helps **control overfitting**.  \n",
    "\n",
    "The input image is partitioned into a set of areas that don’t overlap.  \n",
    "Max pooling is all about grabbing the **maximum value** at each spot in the image.\n",
    "\n",
    "![max pooling](https://thumbs.gfycat.com/FirstMediumDalmatian-size_restricted.gif)\n",
    "\n",
    "Take the **feature map**, apply a **pooling layer**, and the result is the **pooled feature map**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Flattening\n",
    "\n",
    "You flatten the pooled feature map into a sequential column of numbers (a **long vector**).  \n",
    "Allows that information to become the **input layer** of an artificial neural network.  \n",
    "![flattening](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Full Connection\n",
    "\n",
    "Add an **artificial neural network** to our convolutional neural network.  \n",
    "\n",
    "The main purpose of the artificial neural network is to combine our features into more **attributes**. These will predict the classes with greater accuracy.  \n",
    "\n",
    "The error is calculated and then **backpropagated**. The weights and feature detectors are **adjusted** to help optimize the performance of the model. \n",
    "\n",
    "we have to understand what weights to apply to the synapses that connect to the output. We want to know which of the previous neurons are **important** for the output.  \n",
    "\n",
    "If, for example, you have two output classes, one for a cat and one for a dog, a neuron that reads 0 is absolutely **uncertain** that the feature belongs to a cat. A neuron that reads 1 is absolutely **certain** that the feature belongs to a cat. In the final fully connected layer, the neurons will read values between 0 and 1. This signifies different levels of certainty. A value of 0.9 would signify a certainty of 90%.  \n",
    "\n",
    "The **fully connected layer** is a traditional **Multi-Layer Perceptron**. It uses a **classifier** in the output layer. The classifier is usually a **softmax** activation function.  \n",
    "Use the features from the output of the previous layer to **classify** the input image based on the training data.  \n",
    "\n",
    "![full connection](https://image.slidesharecdn.com/f3lzh4yytdq6oxh0ow1u-signature-a8b8daa82312c33588e8ca91d8c4fc770e891e421c440e8374323d4726539ed0-poli-180817161347/95/deep-learning-az-convolutional-neural-networks-cnn-step-4-full-connection-2-638.jpg?cb=1534525145)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax & CrossEntropy\n",
    "\n",
    "### SoftMax \n",
    "Function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities.  \n",
    "Each component will be in the interval (0,1), and the components will add up to 1, so that they can be interpreted as **probabilities**.  \n",
    "\n",
    "![softmax](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/76_blog_image_1.png)\n",
    "\n",
    "### CrossEntropy (Loss function)\n",
    "We want to minimize the loss function so we can maximize the performance of our network.  \n",
    "\n",
    "Way to measure the **difference** between predicted probabilities and ground-truth probabilities, and during training we try to tune parameters so that this difference is minimized.  \n",
    "\n",
    "![crossentropy](https://wikimedia.org/api/rest_v1/media/math/render/svg/c6b895514e10a3ce88773852cba1cb1e248ed763)\n",
    "\n",
    "Minimizing cross entropy is equivalent to minimizing the negative log likelihood of our data.  \n",
    "\n",
    "![loss function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
