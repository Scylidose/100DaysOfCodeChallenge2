{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Data Science and Analytics\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "## Algorithms in Machine Learning\n",
    "### Massive Data Sets\n",
    "- Dealing with massive data sets requires the use of efficient data structures  \n",
    "- Need to compute summaries and samples of data  \n",
    "- Need randomized algorithms  \n",
    "- Need to store data so that it can be efficiently accessed (*binary trees*, *hashing*, ...)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of learning problems \n",
    "#### Supervised\n",
    "**Marked patterns are available**  \n",
    "Task of learning a function that maps an input to an output based on example input-output pairs.  \n",
    "\n",
    "#### Unsupervised\n",
    "**No marked patterns**  \n",
    "Helps find previously unknown patterns in data set without pre-existing labels.  \n",
    "\n",
    "#### A machine learning method is a computer algorithm that searches for patterns in data\n",
    "- Patterns are learned from data  \n",
    "- Mathematical tools for describing patterns: Statistics and probability  \n",
    "- Machine Learning combines these with optimization, algorithms, and other tools  \n",
    "\n",
    "**f(state, action) = next state**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Predicts the category which the data belongs to.  \n",
    "\n",
    "![classification](https://cdn-images-1.medium.com/max/788/1*I4zVi0P_jUaq2FmYvMQg1Q.jpeg)\n",
    "\n",
    "- Need to compute **distances** and minimum distances between a point and a line (**shortest path**)  \n",
    "- Computing a **linear separator** (*classifier*) is a linear programming problem  \n",
    "- Computing a **non-linear separator** (*support vector machine*) requires algorithms for problems such as **semi-definite programming** or **convex programming**\n",
    "\n",
    "##### Feature space\n",
    "- Data = points in ***R***  \n",
    "- Dimensions = scalar measurements  \n",
    "\n",
    "##### Classifier functions\n",
    "- A classifier for K classes is a function  \n",
    "    - f(***R***) = {1, ..., K}  \n",
    "- Classifiers carve up the space feature into regions \n",
    "\n",
    "#### Quantifying mistakes\n",
    "- Loss function for K classes : \n",
    "    - loss : {1, ..., K} x {1, ..., K} -> [0, ∞)  \n",
    "    - loss(f(x), true class of x)\n",
    "    - If all mistakes are equally bad :  \n",
    "        - loss(i, j) = { 1 if i != j; 0 if i = j}\n",
    "- If class distributions known :  \n",
    "    - Risk of classifier is the expected loss  \n",
    "        - risk(f) = E[loss(f(X), true class of X)]  \n",
    "\n",
    "    \n",
    "### Nearest Neighbor Classification\n",
    "Use training data as classifier  \n",
    "- Given : Data point x  \n",
    "- Find training data point closest to x  \n",
    "- Assign x the label of closest point\n",
    "\n",
    "#### Drawbacks\n",
    "- In large data set, finding nearest data points is expensive  \n",
    "- Expense also grows with dimension\n",
    "- Was an important method when data sets were small  \n",
    "\n",
    "### K-means Clustering\n",
    "**Clustering algorithm**.  \n",
    "K represents the **number of clusters** we are going to classify our data points into.  \n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*K3DzIBwc6jlBuGM0W0vYjQ.png\" alt=\"k-means\" width=\"400\"/>\n",
    "\n",
    "- Requires :  \n",
    "    - Shortest paths  \n",
    "    - Randomization  \n",
    "    - Graph models  \n",
    "    - Approximating NP-complete problems  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifiers\n",
    "Classification decision based on the value of a **linear combination** of the characteristics.  \n",
    "\n",
    "#### Hyperplane\n",
    "Create support vector machines.  \n",
    "Used to define decision boundaries.  \n",
    "![hyperplane](https://images.deepai.org/glossary-terms/3bb86574825445cba73a67222b744648/hyperplane.png)  \n",
    "\n",
    "#### Limitations\n",
    "- Problem 1 : Curved optimal decision boundary :  \n",
    "    - Can be addressed using the **kernel trick**  \n",
    "- Problem 2 : Classes may overlap SVM address :  \n",
    "    - Permitting misclassified training points\n",
    "    - Each such point contributes a *cost* to the optimization target function  \n",
    "- Problem 3 : More than two classes :  \n",
    "    - Can be addressed by combining multiple linear classifiers  \n",
    "    - There are several ways to do so; each has drawbacks  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Classifiers\n",
    "- Train many *weak* classifierrs  \n",
    "- Combine results by majority vote  \n",
    "\n",
    "#### Error rate\n",
    "- Proportion of misclassified points  \n",
    "- Expected number of errors\n",
    "\n",
    "#### Weak Classifier\n",
    "- Consider two classes of equal size  \n",
    "- Assigning class by coin flip : *50 %* expected error  \n",
    "- Weak classifier = error rate sightly above 50 %  \n",
    "\n",
    "#### Classification by majority vote \n",
    "- **m classifiers** take a vote, m is an odd number  \n",
    "- Two choices :  \n",
    "    - One is correct  \n",
    "    - One is wrong  \n",
    "- Decision is made by simple majority  \n",
    "\n",
    "For two classes and classifiers f1, ..., fm :\n",
    "    - Majority vote at input x = sgn(∑fi(x))\n",
    "\n",
    "#### Tree Classifiers \n",
    "Uses a **decision tree** to go from observations about an item to conclusions about the item's target value.  \n",
    "![tree classifier](https://upload.wikimedia.org/wikipedia/commons/2/25/Cart_tree_kyphosis.png) \n",
    "\n",
    "#### Training\n",
    "- Input n training points of classes 1, ..., K  \n",
    "    - Select n points uniformly at random with replacement  \n",
    "    - Train a tree on the randomized data set  \n",
    "- Repeat m times  \n",
    "\n",
    "#### Empirical observation\n",
    "    - Tree ensemble typically performs reasonably well  \n",
    "    - Too dependent\n",
    "    \n",
    "#### Tree training\n",
    "    - In each step, computes best split point along each axis  \n",
    "    - Then splits the axis that minimized error  \n",
    "    - Split is optimized over all axes  \n",
    "    \n",
    "#### Random Forests\n",
    "Constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.  \n",
    "- In each step, select small random subset of axes  \n",
    "- Only optimize over those  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "### Terminology\n",
    "- Model  \n",
    "    - Family of classifiers\n",
    "- Parameter  \n",
    "    - Indexes different classifiers within model  \n",
    "- Hyperparameter  \n",
    "    - Indexes different models  \n",
    "\n",
    "#### Training error of a classifier\n",
    "Training error = number of misclassified training points / number of training points  \n",
    "\n",
    "#### Prediction error of a classifier\n",
    "- The distribution of the underlying data source  \n",
    "    - Prediction error = E[proportion of misclassified points]  \n",
    "    - If we assume all errors cost the same, this is the risk  \n",
    "\n",
    "#### Classifier parameters\n",
    "- Tree classifier  \n",
    "    - Number of splits  \n",
    "- Tree ensembler  \n",
    "    - Tree parameters  \n",
    "    - Number of trees  \n",
    "- Random Forest  \n",
    "    - Tree parameters  \n",
    "    - Number of trees  \n",
    "    - Number of random dimension\n",
    "\n",
    "### How do we select an adequate model based on simple data\n",
    "- Model selection chooses a model complexity (*hyperparameter*)  \n",
    "- Training a classifier chooses parameter values  \n",
    "- The training can often be formulated as minimizing the training error  \n",
    "- Model selection **cannot** be performed by minimizing training error, that would lead to overfitting\n",
    "\n",
    "#### If we knew underlying distribution\n",
    "- Train classifiers with different hyperparameters on training data  \n",
    "- Compute prediction errors under true distribution  \n",
    "- Choose the one with smallest prediction error  \n",
    "\n",
    "**Separating model selection and training prevent** ***overfitting*** \n",
    "\n",
    "#### Approximation by sample data \n",
    "- Split training data set  \n",
    "- Train on set 1  \n",
    "- Test predictive performance on set 2  \n",
    "\n",
    "Data splitting estimates the prediction error from data.  \n",
    "Prediction error estimates can be used in two ways :  \n",
    "    - Model selection  \n",
    "    - Classifier assessment  \n",
    "\n",
    "Classifier assessment estimates the prediction error of the final choice of classifier.  \n",
    "\n",
    "- Model selection -> Optimize performance  \n",
    "- Classifier assessment -> Interpret performance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "**Cross validation selects model and assesses classifier**\n",
    "\n",
    "- Split data into *three* sets :  \n",
    "    1) Training set  \n",
    "    2) Test set  \n",
    "    3) Validation set (*hold-out set*)\n",
    "- Train classifiers with different hyperparameters on training set  \n",
    "- Select the one with smallest prediction error on test set  \n",
    "- Estimate performance on validation set  \n",
    "\n",
    "**Prediction error estimate on test set is confounded by model selection**  \n",
    "\n",
    "### How to split the data\n",
    "- If samples assumed -> split at random (samplit without replacement)  \n",
    "- How large should each set be ? \n",
    "    - Large training set -> More accurate classifier  \n",
    "    - Small training set -> Reflects variation between sample sets  \n",
    "    \n",
    "![cross validation](https://3gp10c1vpy442j63me73gy3s-wpengine.netdna-ssl.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-21-at-4.26.53-PM.png)\n",
    "\n",
    "### K-fold cross validation\n",
    "- Remove validation set and set it aside  \n",
    "- Subdivide remaining data into K equally sized blocks\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For k = 1, ..., K\n",
    "    Remove block k from training data\n",
    "    Train classifier on remaining blocks\n",
    "    Estimate prediction error on block k, estimates over all k, and select best classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When classifier chosen, estimate its performance on validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Applications\n",
    "## Probabilistic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
